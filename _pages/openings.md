---
title: "GrunnSec Research Group - Join Us"
layout: textlay
excerpt: "Openings"
sitemap: false
permalink: /vacancies
---

<h1 class="sapienza-text"> Open positions</h1>

We are always looking for new group members with passion, talent, and grit!


<!-- <h3 class="sapienza-text">  Current open positions</h3>
<b>Sapienza University of Rome - PhD in Engineering in Computer Science.</b><br>
Public Call 2024: <a href="https://www.uniroma1.it/en/pagina/admissions-2024-2025-phd-programmes">https://www.uniroma1.it/en/pagina/admissions-2024-2025-phd-programmes</a><br>
PhD Website 2024: <a href="https://www.uniroma1.it/en/pagina/admissions-2024-2025-phd-programmes">https://www.uniroma1.it/en/pagina/admissions-2024-2025-phd-programmes</a><br><br>

<b>Sapienza University of Rome - National PhD in Artificial Intelligence.</b><br>
Public Call 2024: Upcoming...<br>
PhD Website 2024: Upcoming...<br>-->


<h3 class="sapienza-text">Applications for PhD and Postdoc positions</h3>
If you are interested in working with us as a PhD student or postdoc, please contact me per email ( f.turkmen[AT]rug(DOT)nl).

Topics:
 <ul>
  <li>security attacks against AI systems</li>
  <li>privacy-preserving, decentralized machine learning</li>
  <li>fuzz testing of software and systems</li>
  <li>authentication and authorization systems</li>
</ul>

<h3 class="sapienza-text"> Master Theses for University of Groningen students</h3>
If you are a Master student at University of Groningen looking for a Master project, contact me per email ( f.turkmen[AT]rug(DOT)nl) or stop by my office. 
<!-- We are interested mainly on thesis projects about robot learning, knowledge acquisition and learning, reinforcement learning or explainable artificial intelligence, but different topics are welcome!
Here you can find a brief list of available topics for master thesis. These are only a limited set, many others are available and we are also open to your proposals. When you contact us, please identify a macro-topic (RL, XAI, CL+XAI, RL+XAI, NeSy).  -->
<!-- <ul>
<li>[XAI] <strong>Everything related to improving XAI methods.</strong></li> 
<li>[RL] <strong>Everything related to improving RL agents.</strong></li> 
<li>[XAI] <strong>Improving Transparent Explainable Logic Layers</strong>: Overcoming limitations of TELL (<a href="https://doi.org/10.3233/FAIA240579">10.3233/FAIA240579</a>)</li> 
<li>[XAI+GNN] <strong>Prototype-based GNNs</strong>: Overcoming limitations of TELL (<a href="https://doi.org/10.1109/TAI.2022.3222618">10.1109/TAI.2022.3222618</a>)</li> 
<li>[XAI+DD] <strong>XAI in Drug Discovery  (collaboration with Rome Center for Molecular Design, Dept. Pharmaceutical Chemistry at Sapienza University of Rome)</strong>: XAI for several tasks in Drug Discovery, such as molecular property prediction and molecular generation (<a href="https://doi.org/10.1007/s10994-023-06369-y">10.1007/s10994-023-06369-y</a>)</li> 
<li>[RL+XAI] <strong>Self-explainable Logic-based Reinforcement Learning</strong>: Using Logic-based Layers (TELL) to develop self-explainable RL agents (<a href="https://openreview.net/forum?id=JVgRSIafCI">https://openreview.net/forum?id=JVgRSIafCI</a>,<a href="https://doi.org/10.3233/FAIA240579">10.3233/FAIA240579</a>, <a href="https://openreview.net/forum?id=ZC0PSk6Mc6">https://openreview.net/forum?id=ZC0PSk6Mc6</a>)</li> 
<li>[RL+XAI] <strong>Self-explainable Reinforcement Learning</strong>: Development of Reinforcement Learning models that are explainable by design (<a href="https://ceur-ws.org/Vol-3518/paper1.pdf">https://ceur-ws.org/Vol-3518/paper1.pdf</a>)</li> 
<li>[CL+XAI] <strong>Real-time analysis of explanation drift</strong>: Development of metrics to use explanation drift as early detector for catastrophic forgetting (<a href="https://doi.org/10.1016/j.neucom.2024.127960">10.1016/j.neucom.2024.127960</a>, <a href="https://doi.org/10.1016/j.conb.2022.102609">10.1016/j.conb.2022.102609</a>)</li> 
<li>[CL+XAI] <strong>XAI-guided replay per spiking neural networks</strong>: Exploit explanations to select samples to store in a replay buffer in order to make it more efficient (<a href="https://doi.org/10.1109/MLSP55844.2023.10285911">10.1109/MLSP55844.2023.10285911</a>)</li> 
<li>[CL+XAI] <strong>Transfer/Continual learning with Graph Concept Whitening (collaboration with Rome Center for Molecular Design, Dept. Pharmaceutical Chemistry at Sapienza University of Rome)</strong>: Test the GraphCW in transfer/continual learning scenarios.
Use the HDAC datasets (11 protein groups, with the first being much larger than the others). This application is of great interest in the pharmaceutical field, as the different proteins are used for very different purposes (e.g., weight loss vs. cancer). Therefore, understanding which properties make a molecule active for one protein (the one related to cancer) rather than another (the one related to weight loss) could make it possible to propose modifications to the proteins so that they become active for the desired target. (<a href="https://doi.org/10.1007/s10994-023-06369-y">10.1007/s10994-023-06369-y</a>)</li> 
<li>[RL] <strong>RL Autocurricula through visual policy inspection with multi-modal LLMs</strong>. Automatic curriculum generation for RL using video-LLMs to directly inspect agents policies and assess learning progress. 
(<a href="https://arxiv.org/abs/2306.01711" target="_blank">2306.01711</a>)</li>
<li>[RL] <strong>Improving task space exploration in open-ended autocurricula</strong>. Improve the task space exploration techniques from score-based Unsupervised Environment Design (UED) methods (e.g. with intrinsic motivation). 
(<a href="https://arxiv.org/abs/2203.01302" target="_blank">2203.01302</a>, <a href="https://arxiv.org/abs/2408.15099" target="_blank">2408.15099</a>)</li>
<li>[RL+NeSy] <strong>Autocurricula of temporally-extended RL tasks for zero-shot instruction-following</strong>. Automatic curriculum generation of temporally-extended RL tasks represented with formal specifications (e.g. reward machines), with the aim of producing agents that can generalize zero-shot to unseen specifications. 
(<a href="https://arxiv.org/abs/2010.03950" target="_blank">2010.03950</a>, 
<a href="https://arxiv.org/abs/1807.06333" target="_blank">1807.06333</a>, 
<a href="https://arxiv.org/abs/2102.06858" target="_blank">2102.06858</a>, 
<a href="https://arxiv.org/abs/2010.03934" target="_blank">2010.03934</a>, 
<a href="https://arxiv.org/abs/2301.07608" target="_blank">2301.07608</a>)</li>
<li>[RL+NeSy] <strong>Natural language instructions to policies via LLMs+NeSy RL</strong>. Training RL agents capable of following natural language instructions by first turning them into formal specifications that are then solved via NeSy RL. 
(<a href="https://arxiv.org/abs/2010.03950" target="_blank">2010.03950</a>, 
<a href="https://arxiv.org/abs/1807.06333" target="_blank">1807.06333</a>, 
<a href="https://arxiv.org/abs/2102.06858" target="_blank">2102.06858</a>)</li>
<li>[RL+NeSy] <strong>Lightweight and transferrable semantic embeddings of temporally extended RL tasks</strong>. Semantic-preserving representations for temporally-extended tasks in RL using kernel PCA and semantic similarity. 
(<a href="https://arxiv.org/abs/2010.03950" target="_blank">2010.03950</a>, 
<a href="https://arxiv.org/abs/1807.06333" target="_blank">1807.06333</a>, 
<a href="https://ceur-ws.org/Vol-3945/paper3.pdf" target="_blank">paper3.pdf</a>, 
<a href="https://arxiv.org/abs/2405.14389" target="_blank">2405.14389</a>)</li>
<li>[NeSy / NeSyRL] <strong> Logically informed autoregressive learning</strong>. Training/ finetuning autoregresive deep models (LLM / RL agents ) with background knowledge in formal languages. (<a href="https://ceur-ws.org/Vol-3779/paper4.pdf" target="_blank">paper4.pdf</a>, 
<a href="https://arxiv.org/abs/2504.13139" target="_blank">2504.13139</a>, <a href="https://arxiv.org/abs/2311.00094" target="_blank">2311.00094</a>, <a href="https://arxiv.org/abs/2312.03905" target="_blank">2312.03905</a>)</li>
</ul> -->


